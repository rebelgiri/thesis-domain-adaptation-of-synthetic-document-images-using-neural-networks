
%\noindent
\justifying
\setlength{\parskip}{1em}


In the last decades, the field of computer vision and computer graphics has made a tremendous amount of progress, especially in the field of domain adaptation. The image-to-image translation is a popular and classic example of domain adaptation, in which the goal is to learn the mapping between a source image and a target image using a training set of aligned image pairs \cite{mirza2014conditional}. \acp{GAN} were among the initial methods used to perform image-to-image translation \cite{pang2021imagetoimage} \cite{goodfellow2014generative}. But, \acp{GAN} suffers from unstable learning and mode collapse \cite{thanhtung2020catastrophic}. Also, for many tasks, aligned or paired training data will not be available, collecting and annotating paired document images is tedious, difficult, time-consuming and costly. Hence, over the years, several supervised and unsupervised image-to-image translation methods are developed. This chapter discusses some of those popular image-to-image translation methods. Also, a brief theoretical comparison of these existing methods is discussed. The motivation behind selecting the approach to solve the addressed problem statement of this thesis is summarized in section \ref{rwsummary}.



\section{Literature Survey}\label{LiteratureSurvey}


Ian J. Goodfellow et al.\ \cite{goodfellow2014generative} proposed a framework of \acp{GAN} in which two models are simultaneously trained. A generative model is trained to learn the data distribution and a discriminative model that predicts the probability that a sample came from the training data rather than a generative model. The idea of training a generative model is to generate realistic data and maximize the probability of a discriminative model making a mistake. The generator learns to generate realistic data and the discriminator learns to distinguish real data from the generator's fake data. The generator is penalized by the discriminator for producing non-credible results. As training starts, the generator begins to create fake data and the discriminator quickly learns to classify that it's fake and the generator is penalized to improve and produce credible results. The generator gets closer to producing output samples that can fool the discriminator as training progresses. Finally, if generator training goes well, the discriminator becomes worse at distinguishing between real and fake samples.  At the end of the training, ultimately, we have a generator model which produces credible results which are similar to real data \cite{goodfellow2014generative}. Authors have trained \ac{GAN} on a range of datasets including MNIST \cite{726791}, Toronto Face Database (TFD) \cite{susskind2010toronto} and CIFAR-10 \cite{krizhevsky2009learning}, also compared against existing methods like \ac{DBN} \cite{bengio2012better}, \ac{Stacked-CAE} \cite{bengio2012better} and \ac{Deep-GSN} \cite{bengio2014deep}. The authors do not claim that the samples generated by \acp{GAN} are better than samples generated by already existed methods \cite{goodfellow2014generative}. Authors believe at least the obtained results are competitive when compared with the better generative models in the literature, which highlights the potential of the generative adversarial framework \cite{goodfellow2014generative}. The first and foremost advantage of \acp{GAN} is computational when compared to other generative models. Adversarial models may have added statistical advantage from the generator network being updated by the gradients flowing through the discriminator using backpropagation, not being updated directly with data examples \cite{goodfellow2014generative}. Backpropagation is an algorithm to calculate gradients of the loss function to update weights of the neurons in the neural network \cite{goodfellow2017deep}. Authors metioned, to prevent the Helvetica Scenario \cite{manisha2019generative}, special care must be taken while training the \acp{GAN}. The generator must not be trained too much without updating the discriminator, training of both models should happen simultaneously. The discriminator must be updated well to provide maximum error to update the generator to produce high-quality images. Usually, \acp{GAN} should produce a wide variety of outputs. In the Helvetica Scenario, the generator collapses, repeatedly producing the same output or a small set of outputs. The Helvetica Scenario is also called Mode Collapse \cite{thanhtung2020catastrophic}.

\begin{figure}
        \begin{center}
 	    \includegraphics[scale=0.25]{images/relatedWorks/GANEvolution.jpg}
	    \caption[Evolution of \acp{GAN} Over the Years.]{Evolution of \acp{GAN} over the years \cite{PavanKumar.2021}.}
	    \label{fig:GANEvolution}
	    \end{center}
\end{figure}

Xudong Mao et al.\ \cite{mao2017squares} proposed another variant of \acp{GAN} called \acp{LSGAN}. In regular \acp{GAN}, the discriminator is hypothesized to be a classifier with the sigmoid cross-entropy loss function \cite{mao2017squares}. They realized the cross-entropy loss function can cause vanishing gradients problems during the learning process \cite{mao2017squares}. Also, regular \acp{GAN} are unstable during the learning process. Hence, the authors proposed \acp{LSGAN} to solve the problem of vanishing gradients and instability, during the learning process. \acp{LSGAN} use the least-squares loss function in the discriminator to solve the problem of vanishing gradients. In \acp{LSGAN}, the fake samples are penalized by the least-squares loss function, which forces the generator to generate samples closer towards to the decision boundary \cite{mao2017squares}. The authors evaluated \acp{LSGAN} using two datasets LSUN \cite{yu2016lsun} and HWDB1.0 (Handwritten Chinese Character Dataset) \cite{6065551}. They discovered, when trained on the LSUN dataset \cite{yu2016lsun}, \acp{LSGAN} generates better quality images than regular \acp{GAN}, \acp{DCGAN} \cite{radford2016unsupervised} and \acp{EBGAN} \cite{zhao2017energybased}. Also, when trained on the Handwritten Chinese Character Dataset, the generated characters were readable and clear. Another experiment was conducted to evaluate the stability of \acp{LSGAN} on a Gaussian mixture distribution dataset, which is designed in literature by Luke Metz et al.\ \cite{metz2017unrolled}. In this experiment, a regular \ac{GAN} and \acp{LSGAN} trained on a \ac{2D} mixture of 8 Gaussian datasets using a simplistic architecture of both generator and the discriminator, which have three fully-connected layers \cite{mao2017squares}. ``\acp{LSGAN} learn the Gaussian mixture distribution effectively and generate a wide variety of samples, but regular \acp{GAN} suffer from mode collapse and generate the single type of samples of the data distribution'' \cite{mao2017squares} and the learning process of \acp{LSGAN} is stable compared to the regular \acp{GAN}.



Phillip Isola et al.\ \cite{isola2018imagetoimage} proposed \acp{cGAN} as a generic solution to numerous image-to-image translation problems. Authors wanted to provide a single solution for multiple types of image-to-image translation problems. \acp{cGAN} learn to map input images to output images using paired training dataset. Traditionally different loss formulations are required to solve different image-to-image translation problems, but \acp{cGAN} can be used as a common generic method to solve distinct image-to-image translation problems \cite{isola2018imagetoimage}. For example, synthesizing photos from label maps \cite{cordts2016cityscapes}, reconstructing objects from edge maps \cite{zhu2018generative} \cite{6909426} and colorizing images \cite{wesley2021colorizing}. In \ac{cGAN} the generative model uses U-Net-based architecture, the U-Net is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks \cite{ronneberger2015unet}. The discriminative model is constructed using a convolutional PatchGAN classifier architecture \cite{demir2018patchbased}. PatchGAN is simply a \ac{CNN} in which each overlapping patch in an image is estimated to be fake or real. It penalizes structure of an image at the scale of image patches \cite{li2016precomputed}. In \acp{cGAN}, both generative and discriminative models are conditioned on some additional information, such as input images and class labels. But unconditional \acp{GAN} are not conditioned on any auxiliary information \cite{isola2018imagetoimage}. Authors found ``L1 loss encourages less blurring compared to the L2 loss'' \cite{isola2018imagetoimage}. Also combining L1 Loss and \ac{cGAN} (L1 + \ac{cGAN}) generates better results compared to combining Unconditional \ac{GAN} and L1 Loss (L1 + \ac{GAN}). The \ac{cGAN} appears to be more effective on the problem where the output is highly detailed or photographic \cite{isola2018imagetoimage}. Authors have released \ac{cGAN} as the pix2pix software and mentioned, \ac{cGAN} has widespread applications and it can be adopted without ease, to solve different image-to-image translation problems without significant parameter modifications. The pix2pix software code is available at \href{https://github.com/phillipi/pix2pix.}{GitHub}. In figure \ref{fig:CGAN} the mapping from edges $\rightarrow$ photo transformation illustrated and both generator and discriminator are conditioned on additional information like input edge map \cite{isola2018imagetoimage}.


\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=0.30]{images/relatedWorks/CGAN.png}
	    \caption[An illustration of training a \ac{cGAN} to map edges $\rightarrow$	 photo transformation.]{An illustration of the training a \ac{cGAN} to map edges $\rightarrow$ photo transformation. In \acp{cGAN}, both discriminator and generator observe the input edge map, unlike Unconditional \acp{GAN} \cite{isola2018imagetoimage}.}
	    \label{fig:CGAN}
	    \end{center}
\end{figure}

Taesung Park et al.\ \cite{park2020contrastive} proposed \ac{CUT} for encouraging content preservation in unpaired image-to-image translation problems by maximizing the mutual information between input and output with patchwise contrastive learning \cite{oord2019representation}. Authors stated, in patchwise contrastive learning, the patchwise contrastive loss is used to minimize the differences between generated image and the input image, at the level of small patches. The task of patchwise contrastive learning is to bring a patch from the generated image, closer to its corresponding input patch in the input image compared to other patches that are present the same input image. The patchwise contrastive learning forces to preserve context and content between the input image and generated image by learning from patches from the same input image rather than other images in the training dataset \cite{park2020contrastive}. They demonstrated, the framework enables one-sided translation in the unpaired image-to-image translation setting while improving quality, consuming less memory and reducing training time \cite{park2020contrastive}. The prime advantage of this method is it does not require lots of memory \cite{8578491} \cite{he2020momentum} or specialized architecture \cite{henaff2020dataefficient} \cite{bachman2019learning}. The contrastive representation is formulated within the same image, hence this method can even be trained on single images, where each domain is having only a single image \cite{park2020contrastive}. The several prior methods like \ac{CycleGAN} \cite{zhu2020unpaired}, \ac{MUNIT} \cite{liu2018unsupervised}, \ac{DRIT} \cite{lee2019drit} and \ac{GCGAN} \cite{fu2018geometryconsistent} were unable to achieve significant results compared to the \ac{CUT} method, on other hand, it often produced higher quality images and more accurate generations with light footprint in terms of training speed and GPU memory usage. Since \ac{CUT} method is one-sided, it is memory efficient and faster compared to prior baselines. Furthermore, the authors also introduced faster and lighter variant \ac{FastCUT}. \ac{FastCUT} is computationally lighter and produces competitive results. The code and models of \ac{CUT} are available at \href{https://github.com/taesungp/contrastive-unpaired-translation}{GitHub}.


\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=1.20]{images/relatedWorks/LearningToClean.jpg}
	    \caption[An illustration of \ac{CycleGAN} transforming the noisy images into clean images and vice versa.]{An illustration of \ac{CycleGAN} -  It has two generators $G_A$ and $G_B$ and two discriminators $D_A$ and $D_B$. The generator $G_A$ transforms the noisy images into clean images and $G_B$ transforms clean images into noisy images using cycle-consistency loss \cite{zhu2020unpaired}. The discriminators $D_A$ and $D_B$ rejects samples generated by $G_A$ and $G_B$ respectively acting as an adversary \cite{sharma2019learning}.}
	    \label{fig:LearningToClean}
	    \end{center}
\end{figure}



Monika Sharma et al.\ \cite{sharma2019learning} are addressing a problem in the scanning process that often results in the introduction of blur due to camera motion, salt and pepper noise, or water markings, shake, wrinkles, coffee stains, or faded data. These artifacts introduce several readability challenges in modern text recognition algorithms, decreasing their performance and accuracy \cite{sharma2019learning}. The current denoising techniques are based on a supervised learning approach using paired training datasets, in which it contains noisy documents with their corresponding cleaned versions of the same noisy document \cite{sharma2019learning}. However, in the real world, such a paired training dataset is not available to train a model, to generate clean documents from noisy versions \cite{zhu2020unpaired}\cite{sharma2019learning}. Hence, in this literature, the authors used \ac{CycleGAN} an unsupervised image-to-image translation technique to transform noisy document images into clean document images. \ac{CycleGAN} is a method to learn a mapping between two data distributions in the absence of paired training dataset \cite{zhu2020unpaired} \cite{sharma2019learning}. They have compared the performance of \ac{CycleGAN} for document cleaning tasks with \ac{cGAN} by training them over the same dataset. The only difference was \ac{CycleGAN} trained using unpaired images and \ac{cGAN} trained using the paired images. They have used \ac{PSNR}\footnote{Peak Signal-to-Noise Ratio: \url{http://www.ni.com/white-paper/13306/en/} last access: \dcdate.} as a evaluation metric to evaluate the quality of transformed denoised images. Experiments were performed on 4 separate public document datasets, one each for deblurring, watermark removal, background noise removal and defading. Finally, they concluded \ac{CycleGAN} learns a more robust mapping from the space of noisy to clean documents compared to \ac{cGAN} \cite{sharma2019learning}. The complete architecture of \ac{CycleGAN} for denoising documents illustrated in figure \ref{fig:LearningToClean}.


\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=0.35]{images/relatedWorks/CycleGANExamples.png}
	    \caption[An illustration of \ac{CycleGAN} transforming an image from one into the other and vice versa.]{An illustration of \ac{CycleGAN} transforming an image from one into the other and vice versa. For example, (a) zebra image transformed into horse image and vice versa and (b) the view of Yosemite mountains in summer transformed into winter and vice versa \cite{zhu2020unpaired}.}
	    \label{fig:CycleganExamples}
	    \end{center}
\end{figure}


Jun-Yan Zhu et al.\ \cite{zhu2020unpaired} noticed the scarcity of paired training data to perform image-to-image translation. Hence, they proposed \ac{CycleGAN} as an image-to-image translation method that can transform the images from source domain $X$ to target domain $Y$ in the absence of paired training dataset. This method learns to capture underlying characteristics of the collection of images in the source domain and figuring out how these characteristics could be transformed into the collection of images in the target domain, all in the absence of any paired training examples \cite{zhu2020unpaired}. In this method, the goal is to learn a mapping $G\ \colon X \rightarrow Y$ so that the generated distribution of images $G(X)$ matches the target distribution $Y$ using an adversarial loss. However, the adversarial losses alone are not sufficient to map the images from the source domain to the target domain. Hence, authors introduced inverse mapping $F\ \colon Y \rightarrow X$ to introduce a cycle-consistency loss to enforce $F(G(X))\approx X$ and vice versa to regularize the output and keep it highly under-constrained \cite{zhu2020unpaired}. The cycle-consistency property of an image-to-image translation can be understood by a simple example, if we translate a sentence of English to French, again back from French to English, one should arrive back to the original sentence. In this manner, \ac{CycleGAN} learns to produce images within the context instead of generating random images\footnotemark. Authors investigated several prior methods like Bi-GAN/ALI \cite{donahue2017adversarial} \cite{dumoulin2017adversarially}, CoGAN \cite{liu2016coupled}, SimGAN \cite{shrivastava2017learning} were unable to achieve compelling results. However, \ac{CycleGAN} often produces images that are equivalent quality to the fully supervised pix2pix software \cite{isola2018imagetoimage}. Authors provided \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{PyTorch} implementation of \ac{CycleGAN}. The figure \ref{fig:CycleganExamples} illustrates, examples of \ac{CycleGAN} transforming an image from one into the other and vice versa.

\footnotetext{\url{https://machinelearningmastery.com/what-is-cyclegan/} last access: \dcdate}

Chris Tensmeyer et al.\ \cite{8978087} realized solving binarization tasks using deep learning models is very challenging. It is due to the scarcity of large quantities of labeled data available to train such models. There have been efforts to create synthetic data for binarization using image processing techniques but, they generally lack realism \cite{8978087}. Hence, authors proposed DGT-CycleGAN to produce realistic synthetic data using an adversarially trained image-to-image translation model \cite{8978087}. Authors modified the \ac{CycleGAN} model to be conditioned on the ground truth binarization mask as it transforms images from the source domain of synthetic images to the target domain of real images \cite{8978087}. They found modifying the discriminator to condition on the binarization \ac{GT} leads to increased realism and better agreement between the \ac{GT} and the produced image \cite{8978087}.  Authors pretrained deep learning models on realistic synthetic datasets generated by \ac{CycleGAN}, DGT-CycleGAN and image compositing \cite{8978087}. They found pretraining a model on realistic synthetic data generated by the DGT-CycleGAN model achieves the best performance for solving binarization tasks compared to data generated by \ac{CycleGAN} and image compositing \cite{8978087}. Also, they evaluated both pretrained only and finetuned models on each of the \ac{DIBCO} datasets\footnotemark to measure their performance \cite{8978087}. Finetuning a model on \ac{DIBCO} dataset after pretraining on the realistic synthetic images generated by the DGT-CycleGAN has lead to a 13\% reduction of F-measure error compared to no pretraining and to 6\% error reduction compared to pretraining on non-realistic synthetic data \cite{8978087}. Authors concluded, pretraining deep neural networks on the realistic synthetic data generated using DGT-CycleGAN lead to better predictive performance both before and after finetuning on real data.

\footnotetext{\ac{DIBCO} 2019: \url{https://vc.ee.duth.gr/dibco2019/} last access: \dcdate}


\section{Summary}\label{rwsummary}
Over the years, \acp{GAN} have evolved to solve different kinds of problems (figure \ref{fig:GANEvolution}). General \acp{GAN} suffer from unstable training, vanishing gradients problem and mode collapse. Therefore, Martin Arjovsky et al.\ \cite{arjovsky2017wasserstein} proposed \acp{WGAN} to solve problems with \acp{GAN}. \acp{WGAN} solve problems like mode collapse, improve learning stability and provide meaningful learning curves beneficial for hyperparameter searches and debugging \cite{arjovsky2017wasserstein}. Although the implementation of \ac{WGAN} is straightforward, the theory behind it is difficult and requires modifications, for example, implementation of Weight Clipping \cite{gulrajani2017improved}. Hence, Xudong Mao et al.\ \cite{mao2017squares} proposed a simple and more intuitive method compared to \ac{WGAN}, called \ac{LSGAN}. First, \acp{LSGAN} generate higher quality images than \acp{GAN} \cite{mao2017squares}. Second, its learning process is stable compared to \acp{GAN} \cite{mao2017squares}. Monika Sharma et al.\ \cite{sharma2019learning} demonstrated \ac{CycleGAN} can transform noisy document images into denoised and clean document images. They demonstrated \ac{CycleGAN} transforms noisy document images into clean document images superior to \ac{cGAN}. Chris Tensmeyer et al.\ \cite{8978087} proposed DGT-CycleGAN, a modified version of \ac{CycleGAN}, which is adequate to translate images from the domain of synthetic images to the domain of real images to solve binarization tasks. Taesung Park et al.\ \cite{park2020contrastive} proposed \ac{CUT}. They have demonstrated that \ac{CUT} is a better, faster and memory-efficient approach to perform unsupervised image-to-image translation. Further, Jun-Yan Zhu et al.\ \cite{zhu2020unpaired} proposed an unsupervised image-to-image translation approach \ac{CycleGAN} to address the problem of scarcity of paired training data while training image-to-image translation applications. Compared to \acp{GAN}, \acp{CycleGAN} also can deal more meticulously with the problems like unstable training, vanishing gradients problems and mode collapse. In this thesis, the image-to-image translation model is developed using \ac{LSGAN} and \ac{CycleGAN}. The generators and discriminators are optimized using cycle-consistency loss \cite{zhu2020unpaired} and least-square loss \cite{mao2017squares}. The proposed method is simple and capable to solve problems in general \acp{GAN}. Also, it learns a function of image-to-image translation in the absence of paired training data using cycle-consistency property \cite{zhu2020unpaired}.



\begin{comment}
\section{Conclusion}\label{rwconclusion}

The image-to-image translation is a classic example of computer vision and computer graphics problems. In which the goal is to learn the mapping between a source image and target image using a training set of aligned image pairs. Although, for many tasks, aligned or paired training data will not be available. Also, collecting and annotating paired document images is tedious, difficult, time-consuming and costly. This thesis is attempting to close a domain gap between synthetic document images and real document images in the absence of paired training data. The thesis proposes an image-to-image translation application to transform synthetic document images into realistic document images to perform domain adaptation, by closing the gap between synthetic data distribution and real data distribution. Ultimately, a large amount of realistic annotated set of document images can be generated using this application. Furthermore, they can be used to improve real document image classification. Also, in case, if a classifier is trained using such generated realistic document images, it can fasten the process of labeling the new annotated or unlabeled real document images. As per the above literature survey \cite{zhu2020unpaired} \cite{sharma2019learning} and problem statement, in this thesis \ac{CycleGAN} is a decided method to transform synthetic document images into realistic document images in the absence of paired training data. Hence, the proposed image-to-image translation application is realized using \ac{CycleGAN}.
\end{comment}





























%Authors considered evaluation metrics like \ac{AMT} perceptual studies, FCN-score \cite{isola2018imagetoimage} and semantic segmentation metrics to compare the quality of generated images against other baseline. 
%The evaluation metrics like \ac{FID} \cite{heusel2018gans} and semantic segmentation scores are used to compare the quality of generated images using \ac{CUT} method. 

%Authors performed multiple experiments during ablation studies using evaluation metrics like, \ac{AMT} perceptual study, FCN-Score and semantic segmentation metrics. 

%In this thesis, \ac{CycleGAN} is used to perform domain adaptation. Discussion about the loss functions has been carried out in-depth in Chapter \ref{methodology}.
%is combined with \ac{LSGAN}, in which the discriminator model is updated using a least-squares loss (L2 Loss).
%The thesis aims to develop an image-to-image translation application to transform synthetic document images into realistic document images. The thesis aims to develop an image-to-image translation application to transform synthetic document images into realistic document images to perform domain adaptation. Ultimately, a large amount of realistic annotated set of document images can be generated using this application.




