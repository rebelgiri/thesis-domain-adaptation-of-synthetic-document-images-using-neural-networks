
%\noindent
\justifying
\setlength{\parskip}{1em}

In the last decades, the field of computer vision has made a tremendous amount of progress, especially in the field of domain adaptation. Numerous image-to-image translation methods are invented and are improved significantly, \acp{GAN} were among them \cite{pang2021imagetoimage}. Further, several variants of \acp{GAN} are evolved over the years to resolve various problems. This chapter aims to discuss different image-to-image translation methods. Also, a brief theoretical comparison of existing methods is carried out in section \ref{rwdiscussion}. Finally, section \ref{rwconclusion} concludes the motivation behind choosing the proposed approach of this thesis.

\section{Literature Survey}\label{LiteratureSurvey}


Ian J. Goodfellow et al.\cite{goodfellow2014generative} proposed a framework of \acp{GAN} in which two models are simultaneously trained. ``A generative model is trained to learn the real data distribution, and a discriminative model that predicts the probability that a sample came from the training data rather than a generative model. The idea of training a generative model is to generate realistic data and maximize the probability of a discriminative model making a mistake. The generator learns to generate realistic data, and the discriminator learns to distinguish real data from the generator's fake data. The generator is penalized by the discriminator for producing non-credible results. As training starts, the generator begins to create fake data, and the discriminator quickly learns to classify that it's fake and the generator is penalized to improve and produce credible results. The generator gets closer to producing output samples that can fool the discriminator as training progresses. Finally, if generator training goes well, the discriminator becomes worse at distinguishing between real and fake samples.  At the end of the training, ultimately, we have a generator model which produces credible results which are similar to real data'' \cite{goodfellow2014generative}. Authors have trained \ac{GAN} on a range of datasets including MNIST \cite{726791}, the Toronto Face Database (TFD) \cite{susskind2010toronto}, and CIFAR-10 \cite{krizhevsky2009learning}. Also compared against already existing methods like \ac{DBN} \cite{bengio2012better}, \ac{Stacked-CAE} \cite{bengio2012better}, and \ac{Deep-GSN} \cite{bengio2014deep}. The authors do not claim that ``the samples generated by \acp{GAN} are better than samples generated by already existed methods'' \cite{goodfellow2014generative}. Authors believe that ``at least these samples are competitive when compared with the better generative models in the literature, which highlights the potential of the generative adversarial framework'' \cite{goodfellow2014generative}. ``Primarily \acp{GAN} have a computational advantage'' \cite{goodfellow2014generative}. ``Adversarial models may have added statistical advantage from the generator network being updated by the gradients flowing through the discriminator using backpropagation, not being updated directly with data examples'' \cite{goodfellow2014generative}. Backpropagation is an algorithm to calculate gradients of the loss function to update weights of the neurons in the neural network \cite{goodfellow2017deep}. To prevent the Helvetica Scenario \cite{manisha2019generative}, special care must be taken while training the \acp{GAN}. For example, without updating the discriminator, the generator must not be trained too much. The discriminator must be updated well to provide maximum error to update the generator to produce high-quality images. In the Helvetica Scenario, the generator collapses, repeatedly producing the same output or a small set of outputs. Usually, \acp{GAN} should produce a wide variety of outputs. The Helvetica Scenario is also called Mode Collapse \cite{thanhtung2020catastrophic}.





\begin{figure}
        \begin{center}
 	    \includegraphics[scale=0.25]{images/relatedWorks/GANEvolution.jpg}
	    \caption[Evolution of \acp{GAN} Over the Years.]{Evolution of \acp{GAN} Over the Years \cite{PavanKumar.2021}.}
	    \label{fig:GANEvolution}
	    \end{center}
\end{figure}



Xudong Mao et al.\cite{mao2017squares} proposed another variant of \acp{GAN} called \acp{LSGAN}. ``The discriminator is hypothesized to be a classifier with the sigmoid cross-entropy loss function in Regular \acp{GAN}. They realized, however, that the cross-entropy loss function can cause vanishing gradients during the learning process. The authors presented the \acp{LSGAN}, which uses the least-squares loss function for the discriminator to solve this problem'' \cite{mao2017squares}. ``The fake samples are penalized by the least-squares loss function, which forces the generator to generate samples closer towards to the decision boundary'' \cite{mao2017squares}. The authors evaluated \acp{LSGAN} using two datasets LSUN \cite{yu2016lsun} and HWDB1.0 (Handwritten Chinese Character Dataset) \cite{6065551}. When trained on the LSUN dataset \cite{yu2016lsun} they observed, the images generated by \acp{LSGAN} are of better quality than the ones generated by the two baseline methods, \acp{DCGAN} \cite{radford2016unsupervised} and EBGANs \cite{zhao2017energybased}. Also, when trained on the Handwritten Chinese Character Dataset, the generated characters were readable and clear. Another experiment was conducted to evaluate the stability of \acp{LSGAN} on a Gaussian mixture distribution dataset, which is designed in literature by Luke Metz et al.\cite{metz2017unrolled}. They train \acp{LSGAN} and regular \ac{GAN} on a 2D mixture of 8 Gaussian datasets using a simple architecture, where both the generator and the discriminator contain three fully-connected layers. ``It is observed that regular \acp{GAN} suffer from mode collapse'' \cite{mao2017squares}.``\acp{GAN} generate samples around a single valid mode of the data distribution. But \acp{LSGAN} learn the Gaussian mixture distribution successfully'' \cite{mao2017squares}. Further, in this paper, for evaluating the stability numerous comparison experiments are conducted and the results demonstrate that \acp{LSGAN} can generate higher quality images than regular \acp{GAN}, \acp{DCGAN} \cite{radford2016unsupervised}, and \acp{EBGAN} \cite{zhao2017energybased} and the learning process is stable compared to the regular \acp{GAN}.


Phillip Isola et al.\cite{isola2018imagetoimage} proposed \acp{cGAN} as a generic solution to numerous image-to-image translation problems. The authors wanted to provide a single solution for multiple types of image-to-image translation problems. For example, synthesizing photos from label maps \cite{cordts2016cityscapes}, reconstructing objects from edge maps \cite{zhu2018generative} \cite{6909426} , and colorizing images. ``\acp{cGAN} learns the mapping from the input image to output image along with a loss function to train this mapping. This allows using the same generic method to the distinct problems, that traditionally would require very different loss formulations'' \cite{isola2018imagetoimage}. Authors implemented \ac{cGAN} and released it as the pix2pix software to solve distinct image-to-image translation problems. ``In \ac{cGAN} the generator uses U-Net-based architecture \cite{ronneberger2015unet}, the U-Net is an encoder-decoder with skip connections between mirrored layers in the encoder and decoder stacks. The discriminator uses a convolutional PatchGAN classifier \cite{li2016precomputed}, which only penalizes structure at the scale of image patches. Unlike unconditional \acp{GAN}, both the generator and discriminator observe the input images'' \cite{isola2018imagetoimage}. Authors performed multiple experiments during ablation studies using evaluation metrics like, \ac{AMT} perceptual study, FCN-Score, and semantic segmentation metrics. They found that ``L1 distance loss encourages less blurring compared to the L2 distance loss'' \cite{isola2018imagetoimage}. Also combining L1 Loss and \ac{cGAN} (L1 + \ac{cGAN}) generates better results compared to combining Unconditional \ac{GAN} and L1 Loss ((L1 + \ac{GAN})). ``The \ac{cGAN} appears to be more effective on the problem where the output is highly detailed or photographic'' \cite{isola2018imagetoimage}. The pix2pix software code is available at \href{https://github.com/phillipi/pix2pix.}{GitHub}. In figure \ref{fig:CGAN} the mapping from edges \textrightarrow photo transformation illustrated, and ``both generator and discriminator are conditioned on auxiliary information like input edge map'' \cite{isola2018imagetoimage}.



\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=0.30]{images/relatedWorks/CGAN.png}
	    \caption[Illustration of training a \ac{cGAN} to map edges \textrightarrow photo transformation.]{Illustration of training a \ac{cGAN} to map edges \textrightarrow photo transformation. Unlike unconditional \acp{GAN} both discriminator and generator observe the input edge map \cite{isola2018imagetoimage}.}
	    \label{fig:CGAN}
	    \end{center}
\end{figure}




Taesung Park et al.\cite{park2020contrastive} proposed a framework for encouraging content preservation in unpaired image-to-image translation problems by maximizing the mutual information between input and output with patchwise contrastive learning \cite{oord2019representation}. ``In the patchwise contrastive learning for image-to-image translation, a generated output patch should appear closer to its corresponding input patch in comparison to other random patches present in the same input. To achieve patchwise contrastive learning, drawing patches internally from within the input image, rather than externally from other images in the dataset, forces the patches to better preserve the content of the input'' \cite{park2020contrastive}. This method requires neither a memory bank nor specialized architecture. The authors demonstrated that ``the framework enables one-sided translation in the unpaired image-to-image translation setting while improving quality, consuming less memory, and reducing training time'' \cite{park2020contrastive}. They call this approach \ac{CUT}. Since contrastive representation is formulated within the same image, this method can even be trained on single images, where each domain is having only a single image. The several prior methods like \ac{CycleGAN} \cite{zhu2020unpaired}, \ac{MUNIT} \cite{liu2018unsupervised}, \ac{DRIT} \cite{lee2019drit}, and \ac{GCGAN} \cite{fu2018geometryconsistent} were unable to achieve significant results compared to the \ac{CUT} method, on other hand, it often produced higher quality images and more accurate generations with light footprint in terms of training speed and GPU memory usage. Since \ac{CUT} method is one-sided, it is memory efficient and faster compared to prior baselines. The evaluation metrics like \ac{FID} \cite{heusel2018gans} and semantic segmentation scores are used to compare the quality of generated images using \ac{CUT} method. Furthermore, the authors also introduced faster and lighter variant \ac{FastCUT}. \ac{FastCUT} also produces competitive results with even lighter computation cost of training. The code and models for \ac{CUT} are available at \href{https://github.com/taesungp/contrastive-unpaired-translation}{GitHub}.


\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=0.60]{images/relatedWorks/LearningToClean.jpg}
	    \caption[Illustration of \ac{CycleGAN} transforming the noisy document images into clean document images. and vice versa.]{Illustration of CycleGAN transforming the noisy document images into clean document images. The generators $G_A$ and $G_B$ are responsible for the mapping of noisy images to clean images using cycle-consistency loss \cite{zhu2020unpaired}. And the two discriminators $D_A$ and $D_B$ rejects samples generated by $D_A$ and $D_B$ acting like an adversary \cite{sharma2019learning}.}
	    \label{fig:LearningToClean}
	    \end{center}
\end{figure}



Monika Sharma et al.\cite{sharma2019learning} is addressing ``a problem in the scanning process that often results in the introduction of blur due to camera motion, salt and pepper noise, or water markings, shake, wrinkles, coffee stains, or faded data. These artifacts pose many readability challenges to current text recognition algorithms and significantly degrade their performance'' \cite{sharma2019learning}. So, ``the existing denoising techniques require a paired training dataset consisting of noisy documents paired with cleaned versions of the same document. However, in the real world, to generate clean documents from noisy versions, such a paired training dataset is not available to train a model'' \cite{sharma2019learning}. Hence, the authors proposed to use \ac{CycleGAN} because ``it is state-of-the-art and known to learn a mapping between the distributions in the absence of paired training dataset. Using \ac{CycleGAN}, noisy document images transformed into denoised and clean document images to achieve image-to-image translation'' \cite{sharma2019learning}. They have compared the performance of \ac{CycleGAN} for document cleaning tasks with a \ac{cGAN} by training them over the same dataset. The only difference was \ac{CycleGAN} trained using unpaired images and \ac{cGAN} trained using the paired images. They have used \ac{PSNR}\footnote{Peak Signal-to-Noise Ratio: \url{http://www.ni.com/white-paper/13306/en/} last access: \dcdate.} as a evaluation metric to evaluate the quality of transformed denoised images. Several experiments were performed on 4 separate public document datasets, one each for deblurring, watermark removal, background noise removal, and defading. Finally, they conclude that ``\ac{CycleGAN} learns a more robust mapping from the space of noisy to clean documents compared to \ac{cGAN}'' \cite{sharma2019learning}. The complete architecture of \ac{CycleGAN} for denoising documents illustrated in figure \ref{fig:LearningToClean}.


\begin{figure}[H]
        \begin{center}
 	    \includegraphics[scale=0.35]{images/relatedWorks/CycleGANExamples.png}
	    \caption[Illustration of \ac{CycleGAN} transforming an image from one into the other and vice versa.]{Illustration of \ac{CycleGAN} transforming an image from one into the other and vice versa. For example, zebra image transformed into horse image, and vice versa. The view of Yosemite mountains in summer transformed into winter, and vice versa \cite{zhu2020unpaired}.}
	    \label{fig:CycleganExamples}
	    \end{center}
\end{figure}



Chris Tensmeyer et al.\cite{8978087} realized solving binarization tasks using deep learning models is very challenging. It is due to the scarcity of large quantities of labeled data available to train such models. They also mention there have been efforts to create synthetic data for binarization using image processing techniques but, they generally lack realism. In this paper, the authors proposed ``a method to produce realistic synthetic data using an adversarially trained image translation model'' \cite{8978087}. They extended ``the popular \ac{CycleGAN} model to be conditioned on the ground truth binarization mask as it translates images from the domain of synthetic images to the domain of real images'' \cite{8978087}. They have found that modifying the discriminator to condition on the binarization \ac{GT} leads to increased realism and better agreement between the \ac{GT} and the produced image. They called the proposed model DGT-CycleGAN. Also shown ``DGT-CycleGAN model produces more realistic synthetic data'' \cite{8978087}. They validated their approach by pretraining deep networks on realistic synthetic datasets generated by DGT-CycleGAN, \ac{CycleGAN}, and image compositing. They evaluate both pretrained only and finetuned models on each of the \ac{DIBCO} datasets. They have concluded that pretraining deep neural networks on the more realistic synthetic data generated using DGT-CycleGAN lead to better predictive performance both before and after finetuning on real data.



Jun-Yan Zhu et al.\cite{zhu2020unpaired} proposed a modified version of \ac{GAN} which is the state-of-the-art method for the image-to-image translation that can transform the images from source domain $X$ to target domain $Y$ in the absence of paired training dataset. ``This method can learn to capture special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. This modified version of \ac{GAN} is called \ac{CycleGAN}. In this method, the goal is to learn a mapping $G\ \colon X \rightarrow Y$ such that the distribution of images $G(X)$ is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained and coupled with an inverse mapping $F\ \colon Y \rightarrow X$ to introduce a cycle-consistency loss to enforce $F(G(X))\approx X$ and vice versa'' \cite{zhu2020unpaired}. Along with an adversarial loss and cycle-consistency loss, this work also introduces identity mapping loss which helps to preserve the colour of input images. Authors considered evaluation metrics like \ac{AMT} perceptual studies, FCN-score \cite{isola2018imagetoimage}, and semantic segmentation metrics to compare the quality of generated images against other baseline. The several prior methods like Bi-GAN/ALI [\cite{donahue2017adversarial}, \cite{dumoulin2017adversarially}], CoGAN \cite{liu2016coupled}, SimGAN \cite{shrivastava2017learning} were unable to achieve compelling results. The \ac{CycleGAN} method, on other hand, can produce images that are often of similar quality to the fully supervised pix2pix \cite{isola2018imagetoimage}. Authors provide both \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{PyTorch} and \href{https://github.com/junyanz/CycleGAN}{Torch} implementations. In figure \ref{fig:CycleganExamples} examples of \ac{CycleGAN} transforming an image from one into the other and vice versa illustrated.



\section{Discussion}\label{rwdiscussion}
Over the years, \acp{GAN} have evolved to solve different kinds of problems. The evolution of the \acp{GAN} is illustrated in the figure \ref{fig:GANEvolution} using a timeline diagram. \acp{GAN} suffer from unstable training, vanishing gradients problem, and mode collapse. Consequently, Martin Arjovsky et al.\cite{arjovsky2017wasserstein} proposed \ac{WGAN} to solve problems with \acp{GAN}. \acp{WGAN} improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Although the implementation of \ac{WGAN} is straightforward, the theory behind it is heavy and requires some hack, for example, Weight Clipping  \cite{gulrajani2017improved}. Hence, Xudong Mao et al.\cite{mao2017squares} proposed a simple and more intuitive method compared to \ac{WGAN}, called \ac{LSGAN}. First, \acp{LSGAN} are able to generate higher quality images than regular \acp{GAN}. Second, \acp{LSGAN} perform more stable during the learning process. Moreover, Jun-Yan Zhu et al.\cite{zhu2020unpaired} proposed \ac{CycleGAN}. It is an unsupervised image-to-image translation approach. Compared to \acp{GAN}, \acp{CycleGAN} can deal more meticulously with the problems like unstable training, vanishing gradients problems, and mode collapse. Also, they described that \ac{CycleGAN} has outperformed existing baselines as Bi-GAN/ALI [\cite{donahue2017adversarial}, \cite{dumoulin2017adversarially}], CoGAN\cite{liu2016coupled}, and SimGAN \cite{shrivastava2017learning}. Furthermore, Monika Sharma et al.\cite{sharma2019learning} proclaimed  \ac{CycleGAN} can transform noisy document images into denoised and clean document images to achieve image-to-image translation. They have also compared the performance of the developed image-to-image application with \ac{cGAN} and demonstrated \ac{CycleGAN} had outperformed \ac{cGAN}. Chris Tensmeyer et al.\ \cite{8978087} proposed DGT-CycleGAN, a modified version of \ac{CycleGAN}, which is adequate to translate images from the domain of synthetic images to the domain of real images to solve binarization tasks. Moreover, Taesung Park et al.\cite{park2020contrastive} proposed \ac{CUT}. They have demonstrated that \ac{CUT} is a better, faster, and memory-efficient approach to perform unsupervised image-to-image translation. It has outperformed \ac{CycleGAN} \cite{zhu2020unpaired}, \ac{MUNIT} \cite{liu2018unsupervised}, \ac{DRIT} \cite{lee2019drit}, and \ac{GCGAN} \cite{fu2018geometryconsistent}. However, due to time constraints, multiple available references, and code repositories, \ac{CycleGAN} has been a choice for this thesis and research to perform unsupervised image-to-image translation.


\section{Conclusion}\label{rwconclusion}

The image-to-image translation is a classic example of computer vision and computer graphics problems. In which the goal is to learn the mapping between a source image and target image using a training set of aligned image pairs. Although, for many tasks, aligned or paired training data will not be available. Also, collecting and annotating paired document images is tedious, difficult, time-consuming, and costly. This thesis is attempting to close a domain gap between synthetic document images and real document images in the absence of paired training data. The thesis proposes an image-to-image translation application to transform synthetic document images into realistic document images to perform domain adaptation, by closing the gap between synthetic data distribution and real data distribution. Ultimately, a large amount of realistic annotated set of document images can be generated using this application. Furthermore, they can be used to improve real document image classification. Also, in case, if a classifier is trained using such generated realistic document images, it can fasten the process of labeling the new annotated or unlabeled real document images. As per the above literature survey [\cite{zhu2020unpaired}, \cite{sharma2019learning}] and problem statement, in this thesis \ac{CycleGAN} is a decided method to transform synthetic document images into realistic document images in the absence of paired training data. Hence, the proposed image-to-image translation application is realized using \ac{CycleGAN}.



%In this thesis, \ac{CycleGAN} is used to perform domain adaptation. Discussion about the loss functions has been carried out in-depth in Chapter \ref{methodology}.
%is combined with \ac{LSGAN}, in which the discriminator model is updated using a least-squares loss (L2 Loss).
%The thesis aims to develop an image-to-image translation application to transform synthetic document images into realistic document images. The thesis aims to develop an image-to-image translation application to transform synthetic document images into realistic document images to perform domain adaptation. Ultimately, a large amount of realistic annotated set of document images can be generated using this application.




