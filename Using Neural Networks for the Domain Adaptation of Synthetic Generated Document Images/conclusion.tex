%\noindent
\justifying
\setlength{\parskip}{1em}



In this chapter, the overview of the thesis is described by outlining the problem statement, related works, and the proposed solution in section \ref{Overview}. The results of the experiments conducted by the proposed image-to-image translation application to reduce the domain gap between synthetic data distribution and real data distribution are concluded in section \ref{Conclusion}. Finally, in section \ref{FutureWork} the limitations and future work of this thesis are discussed.


\section{Overview}\label{Overview}

Since the last decades, there are many success stories about deep learning are written. Also, deep learning has evolved tremendously because of the introduction of neural networks to learn complex patterns present in videos, images, speeches, etc. The neural networks are widely used in autonomous vehicles, language translations, object detection, face recognition, speech recognition, and many other sophisticated applications. However, there are limitations to deep learning models. Training neural networks require a large amount of data. In traditional deep learning models, neural networks are trained and tested on similar data distribution or the same domain to achieve the final objective by minimizing the error. In real-world scenarios, the data could arrive from different feature spaces, data distributions, and different domains. In such situations, neural networks don't generalize well to the data arrived from new data distribution or domain; for example, the model to detect pedestrian trained images of a pedestrian in summer might fail to detect pedestrians in winter. This problem of performance degradation causes because of domain shift \cite{farahani2020brief}. 

Large amount of training data is required to train neural networks to achieve low generalization error and high performance. However, training data are scarce and it is very tedious job collect the data. Hence, machine learning engineers necessitated to create synthetic data, but neural networks trained using synthetic data will not generalize well on real data. So, over the year several domain adaption methodologies are developed by the machine learning researchers to solve the problem of domain shift. For example, transforming image of zebra from the source domain into image of horse in the target domain, and vice versa. This is achieved by numerous image-to-image translation applications. This thesis aims to solve the problem of data scarcity of real document images in the target domain using an image-to-image translation application. The proposed image-to-image translation application is developed using \ac{CycleGAN} to transform synthetic document images into realistic document images to close the domain gap between synthetic data distribution and real data distribution. The synthetic document images created using templates and handwritten crops are transformed into realistic document images to tackle the problem of data scarcity in the target domain further to improve the classifier of real document images. 



\section{Conclusion}\label{Conclusion}

In previous chapter \ref{evaluation}, in section \ref{results}, qualitative and quantitative results are described. The qualitative results looks very convincing. Images generated by the \ac{CycleGAN} are distinct also they are generated without causing the mode collapse, but there are some typical failure cases observed in the generated images. In section \ref{FailureCases}, typical failure cases of the proposed image-to-image translation application are Illustrated.





\section{Future Work}\label{FutureWork}

Every research work has a specified amount of time assigned to it. This thesis also has inevitable time constraints and limitations. So many experiments are left to be for future investigation. In this thesis, the proposed image-to-image translation application is implemented only using \acp{CycleGAN}. Comparison between other methodologies and chosen methodology \ac{CycleGAN} to transform synthetic document images into realistic document images is left for future research. For example \ac{CUT}, \ac{FastCUT}, \ac{ACL-GAN}, and \ac{DCGAN} are recommended for future research. \acp{GAN} are a great success in generating realistic images, but the training process is not easy; the training is slow and unstable. \acp{GAN} are hard to train. It has problems like mode collapse, vanishing gradients, lack of proper evaluation metric, and hard to converge. The most important problem with \acp{GAN} is there no proper evaluation metric, without a good evaluation metric, it is like working in darkness. \acp{GAN} have complex object functions observing training progress is difficult, there is no signal to where to stop the training, also, there no single common and good performance indicator to evaluate numerous types of \acp{GAN}. In future, finding a proper performance indicator for \acp{GAN} can a part of research. In this thesis, \ac{CycleGAN}'s generators and discriminators are optimized using least-square loss function \cite{mao2017squares} to avoid mentioned problems. The quality of generated images possibly would have been better if generators and discriminators of \ac{CycleGAN} would have optimized using the Wasserstein metric. Wasserstein distance metric is popularly used in \acp{WGAN}. \acp{WGAN} are intended to improve \ac{GAN}'s training by adopting a smooth Wasserstein distance metric for measuring the distance between two probability distributions. Shrivastava et al.\cite{shrivastava2017learning} proposed a strategy for stable training. In which the discriminators are updated using a history of previously generated images instead of the ones produced recently by the generators. An image buffer of maintained to store 50 previously generated images. In the future, this strategy can be used to improve stability during the training and reduce model oscillations.
































%Wasserstein metric provides a smooth measure, which is super helpful for a stable learning process using gradient descents.
%Sadly, Wasserstein GAN is not perfect. Even the authors of the original WGAN paper mentioned that “Weight clipping is a clearly terrible way to enforce a Lipschitz constraint” (Oops!). WGAN still suffers from unstable training, slow convergence after weight clipping (when clipping window is too large), and vanishing gradients (when clipping window is too small).
%Second, to reduce model oscillation [15], we follow Shrivastava et al.’s strategy [46] and update the discriminators using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.
%Wasserstein metric provides a smooth measure, which is super helpful for a stable learning process using gradient descents.
%The most important problem with \acp{GAN} is there no proper evaluation metric, without a good evaluation metric, it is like working in darkness. \acp{GAN} have complex object functions observing training progress is difficult, there is no signal to where to stop the training, also, there no single common and good performance indicator to evaluate numerous types of \acp{GAN}.
%Lack of a proper evaluation metric Generative adversarial networks are not born with a good objection function that can inform us the training progress. Without a good evaluation metric, it is like working in the dark. No good sign to tell when to stop; No good indicator to compare the performance of multiple models.